1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?

Если речь идет о решающих деревьях при реализации алгоритмов градиентного бустинга, то для снижения степени переобчения среди прочих необходимо использовать такие параметры как learning_rate, max_depth, subsample, max_features, n_estimators (iterations). Learning rate это аналог параметра регуляризации l1 /l2 в линейных моделях. Регулируя данный параметр мы снижаем либо повышаем вклад решающего дерева результат работы модели (штрафуем), который в градиентном бустинге достигается последовательным применением слабых моделей и улучшением результата предыдущих слабых моделей - деревьев. Fm(x) = Fm-1(x) + L*hm(x), где F - функция целевой переменной, x - образец, h - дерево, m - количество деревьев, L - learning rate. Тем самым повышается монотонность минимизируемой целевой функции, обучение замедляется, снижается степень переобучения модели. В сочетании с subsampling (случайный заданный выбор отдельного количества образцов), max_features (признаков) степень переобучения снижается еще более эффективно.


2. По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
Для RandomForest
Важность признака определяется путем определения значения вклада данного признака в минимизацию целевой функции.
Рассчитывается как (нормализованное) общее значение минимизации (снижения) целевой функции, вносимое данным признаком.
Уменьшение величины ошибки в узле, взевешенное по вероятности достижения данного узла. Вероятность - отношение количества образцов, достигших данного узла, к общему числу образцов. Чем выше значение, тем важнее признак.

GradientBoosting
Чем чаще признак используется при разделении узла дерева, тем он важнее. Экстраполируя данный подход на ансамбль деревьев получаем важность признака для модели в целом.
Иначе говоря.
Отдельные деревья решений по сути выполняют выбор признаков, выбирая соответствующие точки разделения. Эта информация может использоваться для измерения важности каждой функции; основная идея такова: чем чаще признак используется в точках разделения дерева, тем важнее этот признак. Это понятие важности может быть распространено на ансамбли деревьев решений путем простого усреднения важности характеристик (признаков) каждого дерева.